{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Sampling: Audio To Generative Art"
      ],
      "metadata": {
        "id": "fPP4Q1mM_Wc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Lee"
      ],
      "metadata": {
        "id": "CaAwcCMRBxu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concept\n",
        "This project expands the concept of sampling, which originated from a musical practice where musicians mixed and matched \"samples\" of pre-existing music to create distinct results. This project extends that practice across mediums: audio is reinterpreted by Stable Diffusion into generative art, and then translated back to audio.\n",
        "Machine learning systems treat audio and images as interchangeable data—arrays that can be reshaped and reinterpreted. This system exploits that property to create a translation chain where unexpected meaning emerges through gaps in conversion.\n"
      ],
      "metadata": {
        "id": "IgwLGuC19ou_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Process:\n",
        "Audio → Spectrogram (visual representation)\n",
        "<br> Spectrogram → Abstract art (via Stable Diffusion)\n",
        "<br> Abstract art → Audio (via data conversion methods)\n",
        "\n",
        "<br> The same source material produces radically different results depending on conversion method. This variability is the point—it reveals how meaning is constructed through our methods of reading data."
      ],
      "metadata": {
        "id": "h1G_EZN390Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is the source code for this project. You **DO NOT need to know any programming to use this system**. Please follow the instuctions carefully."
      ],
      "metadata": {
        "id": "2cSxapCq-A2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "Navigate to Edit > Run All to start using this system. Connect to a T4 GPU runtime.  "
      ],
      "metadata": {
        "id": "xQcWA0F9-Wlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install dependencies ---\n",
        "!pip install torch torchaudio torchvision diffusers transformers accelerate safetensors pillow matplotlib --quiet"
      ],
      "metadata": {
        "id": "EwfWg1u68Onf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchvision.transforms as vtrans\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YNY_9fXx8z-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Upload\n",
        "\n",
        "Upload an audio file. You are free to upload any format as long as it is an audio file, but non-wav files will be converted to wav in the following cell."
      ],
      "metadata": {
        "id": "D9n42N-4-m_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Upload any audio file ---\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "audio_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "Atb-llcL8217"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert non-wav files into wav if necessary\n",
        "# Check if the uploaded file is already a WAV\n",
        "if not audio_path.lower().endswith('.wav'):\n",
        "    print(f\"Converting '{audio_path}' to WAV format...\")\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # Define a new WAV file path\n",
        "    # Using os.path.splitext to get base name and then append .wav\n",
        "    base_name = os.path.splitext(audio_path)[0]\n",
        "    new_audio_path = f\"{base_name}.wav\"\n",
        "\n",
        "    # Save as WAV\n",
        "    torchaudio.save(new_audio_path, waveform, sample_rate)\n",
        "\n",
        "    # Update audio_path to point to the new WAV file\n",
        "    audio_path = new_audio_path\n",
        "    print(f\"Conversion complete. New audio path: '{audio_path}'\")\n",
        "else:\n",
        "    print(f\"File '{audio_path}' is already a WAV file. No conversion needed.\")"
      ],
      "metadata": {
        "id": "BXypombJA5M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio -> Spectrogram\n",
        "\n",
        "The following code extracts a spectrogram using standard signal processing (torchaudio).\n",
        "\n",
        "### What's a Spectrogram?\n",
        "A spectrogram is a time-frequency representation of audio:\n",
        "\n",
        "X-axis: time\n",
        "Y-axis: frequency\n",
        "Intensity: amplitude (rendered as brightness/color)\n",
        "\n",
        "This creates a visual \"map\" of sound. When fed to Stable Diffusion—an AI trained on images, not audio—the model interprets these patterns as visual information and transforms them according to its training, with no awareness of the sonic origin."
      ],
      "metadata": {
        "id": "uGuhTnoqAHZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejUQXkyW7mad"
      },
      "outputs": [],
      "source": [
        "# --- Define helper functions ---\n",
        "def get_spectrogram_image(audio_path):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    spec = T.Spectrogram(n_fft=512)(waveform)\n",
        "    if spec.shape[0] > 1:\n",
        "        spec = spec.mean(dim=0, keepdim=True)\n",
        "    spec_db = 10 * torch.log10(spec + 1e-9)\n",
        "    spec_db = (spec_db - spec_db.min()) / (spec_db.max() - spec_db.min())\n",
        "    spec_np = (spec_db.squeeze(0).numpy() * 255).astype(np.uint8)\n",
        "    rgb_img = Image.fromarray(spec_np).convert(\"RGB\")\n",
        "    return rgb_img.resize((400, 300))\n",
        "\n",
        "# --- Generate spectrogram image ---\n",
        "spec_image = get_spectrogram_image(audio_path)\n",
        "\n",
        "print(\"Normalized Spectrogram\")\n",
        "spec_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup device and model ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)"
      ],
      "metadata": {
        "id": "JS-LY3UM_4VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectrogram -> Image\n",
        "\n",
        "Stable Diffusion recreates the spectrogram as abstract art. I deliberately prompted for abstract art to maximize room for interpretation. I also wanted to avoid associations with real-life objects or people.\n",
        "\n",
        "### Avenues for Experimentation:\n",
        "\n",
        " - Changing Stable Diffusion prompts (you can change the prompt in double quotation marks below)\n",
        "\n",
        " - Hyperparameter Tuning\n",
        "    - Guidance Scale: Determines how much influence the prompt has over the outcome. Works from 1 or higher. Defaults to 7.5\n",
        "    - Strength: Between 0 and 1. Determines how much the original image (ie spectrogram) is supposed to change.\n",
        "    - Negative Prompt: Things you don't want in the outcome.\n",
        "\n",
        "Examples: Change the prompt to \"acrylic art\". Lower the strength to a value below 0.5."
      ],
      "metadata": {
        "id": "7UXEdE9G_HJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompt and generation ---\n",
        "prompt = (\n",
        "   \"caffeinated thoughts of a tired college student pulling their 2nd all nighter flying saucers and jellyfish living forever spicy pumpkin coffee abstract image no concrete objects\"\n",
        ")\n",
        "\n",
        "result = pipe(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=\"no letters, no people, no recognizable objects\",\n",
        "    image=spec_image,\n",
        "    strength=0.9,\n",
        "    guidance_scale=2.5,\n",
        ").images\n",
        "\n",
        "# --- Save and show result ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"generative_art_from_{audio_path}_at_{timestamp}.png\"\n",
        "result[0].save(filename)\n",
        "\n",
        "print(f\"✅ Saved as {filename}\")\n",
        "display(result[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "pcmblLYh_5yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image -> Audio\n",
        "\n",
        "This is where multiple interpretive possibilities open up. The abstract art gained from the previous step is a color image, which comes with an RGB color channel. However, audio waveforms only require one-dimensional amplitude information.\n",
        "\n",
        "This surplus of visual information creates a translation problem that's simultaneously technical and artistic: which information gets used, and how?\n",
        "\n",
        "The simplest approach averages all three color channels into a single amplitude stream. But there are a wide range of other methods available. One such approach is based on scipy's chirp function.\n",
        "\n",
        "Each channel can be assigned a distinct sonic parameter (red channel → pitch, green → amplitude, blue → duration, etc.) and combined into composite audio.\n",
        "\n",
        "These choices might appear purely technical, but they carry interpretive weight: why should red correspond to pitch rather than green? What does that mapping mean?"
      ],
      "metadata": {
        "id": "-jgMwN6X_GCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert image back into sound\n",
        "img = result[0].convert(\"RGB\")\n",
        "print(type(img))\n",
        "to_tensor = vtrans.ToTensor()\n",
        "rgb = to_tensor(img)  # [3, H, W]\n",
        "\n",
        "# Convert each channel to spectrogram magnitude\n",
        "def invert_channel(channel):\n",
        "    spec = torch.pow(10.0, (channel * 80 - 80) / 20.0)\n",
        "    freq_bins = channel.shape[0]\n",
        "    n_fft = (freq_bins - 1) * 2\n",
        "    hop_length = n_fft //2\n",
        "    griffin_lim = T.GriffinLim(n_fft=n_fft, hop_length=hop_length)\n",
        "    reconstructed_waveform = griffin_lim(spec)\n",
        "    return reconstructed_waveform\n",
        "\n",
        "waveforms = [invert_channel(c) for c in rgb]\n",
        "\n",
        "waveforms[0]\n"
      ],
      "metadata": {
        "id": "T-DXXhhJwkG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 1: outcome from the red channel only\n",
        "for channel in rgb:\n",
        "    waveform = invert_channel(channel)\n",
        "    print(waveform.size())\n",
        "    max_val = waveform.abs().amax()\n",
        "    if max_val > 0:\n",
        "        waveform = waveform / max_val\n",
        "\n",
        "    print(waveform.unsqueeze(0).shape)\n",
        "    torchaudio.save(\"red_channel.wav\", waveform.unsqueeze(0), 24000)\n",
        "\n",
        "    from IPython.display import Audio\n",
        "\n",
        "    display(Audio(\"red_channel.wav\"))\n",
        "    break\n"
      ],
      "metadata": {
        "id": "EOb7JQ434ITN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take an average of R, G, and B channels\n",
        "# Stack all waveforms into a single tensor: [C, T]\n",
        "stacked = torch.stack(waveforms, dim=0)\n",
        "\n",
        "# Average across channels\n",
        "mixed = stacked.mean(dim=0)\n",
        "\n",
        "# Normalize safely\n",
        "max_val = mixed.abs().amax()\n",
        "if max_val > 0:\n",
        "    mixed = mixed / max_val\n",
        "\n",
        "import torchaudio.functional as F\n",
        "\n",
        "#make audio file longer\n",
        "def stretch_audio(original_audio, stretch_rate):\n",
        "    stretched = F.phase_vocoder(original_audio.unsqueeze(0), rate=1/stretch_rate, phase_advance=original_audio.size()[0] * stretch_rate)\n",
        "    return stretched\n",
        "\n",
        "stretched = stretch_audio(mixed, 3)\n",
        "\n",
        "torchaudio.save(\"rgb_avg.wav\", stretched.to(torch.float32), 24000)\n",
        "\n",
        "# Play output audio\n",
        "Audio(\"rgb_avg.wav\")"
      ],
      "metadata": {
        "id": "kllT6Jo50yv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 2:\n",
        "from scipy.signal import chirp\n",
        "\n",
        "img_np = np.array(result[0].resize((400, 300))) / 255.0\n",
        "height, width, _ = img_np.shape\n",
        "duration = 30  # seconds\n",
        "sample_rate = 24000\n",
        "samples = np.linspace(0, duration, int(sample_rate*duration))\n",
        "\n",
        "audio = np.zeros_like(samples)\n",
        "for x in range(width):\n",
        "    col = img_np[:, x, :].mean(axis=0)\n",
        "    freq = 800 * col[0]   # red → pitch\n",
        "    amp = 0.2 + 0.8 * col[1]    # green → amplitude\n",
        "    tone = amp * chirp(samples, f0=freq, f1=freq * 1.5, t1=duration, method='linear')\n",
        "    audio += tone / width\n",
        "\n",
        "# Convert NumPy array to PyTorch tensor and add a channel dimension\n",
        "audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "torchaudio.save(\"chirp_file.wav\", audio_tensor, 24000)\n",
        "\n",
        "#Play audio\n",
        "Audio(\"chirp_file.wav\")"
      ],
      "metadata": {
        "id": "e0umVMYe16Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual Framework\n",
        "\n",
        "This project operates at the intersection of several questions:\n",
        "\n",
        " - On sampling: If hip-hop sampling recontextualizes music, what happens when we sample across modalities? Is it still sampling if the outcome passes through a different format?\n",
        "\n",
        "  - On ownership: Who has the right to claim work coming out of this system? The person who created the original audio? This system itself? The user?\n",
        "\n",
        " - On translation: Every conversion (audio→visual→audio) is interpretive, not neutral. The technical choices I make about how to read data determine what meaning survives the translation.\n",
        "\n",
        " - On machine learning as medium: AI/ML systems don't distinguish between audio and visual data. Both are multidimensional arrays. I exploited this interchangeability as a creative opening.\n",
        "\n",
        " - On multiplicity: Different conversion methods extract different \"meanings\" from identical data."
      ],
      "metadata": {
        "id": "HHqif2bgDv64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes\n",
        "\n",
        "- Each run is unique due to Stable Diffusion's stochastic sampling.\n",
        "\n",
        "- All result files are thrown out at the end of each session. Save outputs before you close the Colab tab or navigate away to a different window. You can download output files by clicking the file icon at the left sidebar and selecting 'Download' from the dropdown menu for each file."
      ],
      "metadata": {
        "id": "eZ-tGHX6EnBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "Created as a final project for Poetics and Protocols of Sampling (Autumn 2025) at the School of Poetic Computation. SFPC is an experimental online school hosting classes that blend art, code, and critical theory.\n",
        "\n",
        "## Contact Info\n",
        "Alex Lee\n",
        "<br> GitHub: https://github.com/seohyeonlee2020\n"
      ],
      "metadata": {
        "id": "i2w9IW4nFJNV"
      }
    }
  ]
}